{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ledX1hQo5kXC"
   },
   "source": [
    "$\\qquad$ $\\qquad$$\\qquad$  **TDA 232 Machine Learning: Home Assignment 3 -- Classification (20 points)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$ **Goal: K-Nearest-Neighbour (Y), Naive-bayes Classifier (D), Support Vector Machine (D), Logistic Regression (Y)**<br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Grader: Yuchong (Y), Divya (D)** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                     **Due Date: 6th May** <br />\n",
    "$\\qquad$ $\\qquad$$\\qquad$                   **Submitted by: Fikri Farhan Witjaksono, 199508224756, fikrif@student.chalmers.se** <br />\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "General guidelines:\n",
    "*   All solutions to theoretical and pratical problems must be submitted in this ipynb notebook, and equations wherever required, should be formatted using LaTeX math-mode.\n",
    "*   All discussion regarding practical problems, along with solutions and plots should be specified in this notebook. All plots/results should be visible such that the notebook do not have to be run. But the code in the notebook should reproduce the plots/results if we choose to do so.\n",
    "*   Your name, personal number and email address should be specified above.\n",
    "*   All tables and other additional information should be included in this notebook.\n",
    "*   **All the answers for theoretical questions must be filled in the cells created for you with \"Your answer here\" below each question, but feel free to add more cells if needed.**\n",
    "*   Before submitting, make sure that your code can run on another computer. That all plots can show on another computer including all your writing. It is good to check if your code can run here: https://colab.research.google.com.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fbqQD9bj6HzP"
   },
   "source": [
    "\n",
    "# Theoretical Questions\n",
    "## 1. K-Nearest-Neighbour Classification (4 pts)\n",
    "### 1.1 Exercise 1 (2 pts)\n",
    "A KNN classifier assigns a test instance the majority class associated with its K nearest training instances. Distance between instances is measured using Euclidean distance. Suppose we have the following training set of positive (+) and negative (-) instances and a single test instance (o). All instances are projected onto a vector space of two real-valued features (X and Y). Answer the following questions. Assume “unweighted” KNN (every nearest neighbor contributes equally to the final vote).\n",
    "\n",
    "![替代文字](https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/knn2.png)\n",
    "\n",
    "a) What would be the class assigned to this test instance for K=1, K=5, K=7 and why? (**1 pt**)\n",
    "\n",
    "b) The classification result is affected by the increasing K, so what will be the maxinum value of K you think in this case? Why? (**1 pt**)\n",
    "(**Hint: After K reaches a certain value, the classification result will not change. Find the value!**)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K0xnH14a0rsj"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "$\\textbf{(a).}$\n",
    "For the particular test instance shown above,\n",
    "\n",
    "$$  K=1 \\rightarrow \\text{1 negative (-) class}   $$\n",
    "\n",
    "It would be assigned due to the fact that there is one nearest neighbor to the test instance which is in the (-) class.\n",
    "\n",
    "$$ K=5 \\rightarrow \\text{3 negative (-) and 2 positive (+) class}  $$\n",
    "\n",
    "It would be assigned due to the fact that there are three nearest neighbor to the test instance which belongs to the (-) class while there are also 2 2nd-nearest neighbor to the the test instance which belongs to the (+) class.\n",
    "\n",
    "$$ K=7 \\rightarrow \\text{3 negative (-) and 4 positive (+) class} $$\n",
    "\n",
    "It would be assigned due to the fact that there are three nearest neighbor to the test instance which belongs to the (-) class while there are also 4 2nd-nearest neighbor to the the test instance which belongs to the (+) class.\n",
    "\n",
    "$\\textbf{(b).}$\n",
    "The maximum value of K would be $\\textbf{K=12}$. It is due to the fact that the amount of members in the smallest class (e.g. (-) class) is 6 members and we double it for maximum K. For $K>12$, there will be $\\textbf{class imbalance}$ where the classification result will always include only the (+) positive class.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FqgZRVS80xr7"
   },
   "source": [
    "### 1.2 Exercise 2 (2 pts)\n",
    "Consider 5 data points:\n",
    "\n",
    "$$\\{({0},{1}), ({-1},{0})\\}∈ Class1,$$ \n",
    "\n",
    "$$\\{({1},{0}), ({0},{-1}), (-\\frac{1}{2}, \\frac{1}{2})\\}∈ Class2.$$\n",
    "\n",
    "Consider two test data points:\n",
    "\n",
    "$$(-\\frac{3}{4}, \\frac{3}{4})∈ Class1, (\\frac{1}{2}, \\frac{1}{2})∈ Class2$$\n",
    "\n",
    "Compute the **probability of error** based on k-nearest neighbor rule when $ K=\\{1, 2, 3, 4, 5\\}$ and explain why.\n",
    "(**Hint: The probability of error is the probability of one point is misclassified times the probability of another point is also misclassified**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pK-7HuAB0ztS"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "![task1 2](https://user-images.githubusercontent.com/58375350/80707830-eb20d800-8aea-11ea-9951-168b21e5d273.png)\n",
    "\n",
    "\n",
    "From above, we could define the probability of misclassification either in test point $(-0.75,0.75)\\in Class1$ denoted as **$P_{A}$** or in test point $(0.5,0.5) \\in Class2$ denoted as **$P_{B}$** as following\n",
    "\n",
    "$$ K=1 \\rightarrow P_{error} =  P_{A} \\times P_{B} =  1 \\times \\frac{1}{2} = 0.5  $$\n",
    "\n",
    "**when K=1**, the test point A will be misclassified anyway, so the P error (A)=1. The test point B has the same distance to Class 1 and Class 2, so it could either be classified rightly or misclassified, then P error (B)=0.5. Thus, the $P=1\\cdot\\frac{1}{2}=\\frac{1}{2}$, when K=1\n",
    "\n",
    "$$ K=2 \\rightarrow P_{error} = P_{A} \\times  P_{B} =  \\frac{1}{2} \\times \\frac{1}{2} = 0.25  $$\n",
    "\n",
    "**when K=2**, the test point A will be classified rightly or misclassified, and the P error (A)=0.5. The test point B has the same distance to Class 1 and Class 2, so it could either be classified rightly or misclassified, then P error (B)=0.5. Thus, the $P=\\frac{1}{2}\\cdot\\frac{1}{2}=0.25$, when K=2\n",
    "\n",
    "$$ K=3 \\rightarrow P_{error} = P_{A} \\times  P_{B} =  \\frac{1}{3} \\times \\frac{1}{3} = 0.11  $$\n",
    "\n",
    "**when K=3**, the test point A will be classified rightly or misclassified, and the P error (A)=1/3. The test point B has the same distance to Class 1 and Class 2, so it could either be classified rightly or misclassified, then P error (B)=1/3. Thus, the $P=\\frac{1}{3}\\cdot\\frac{1}{3}=0.11$, when K=3\n",
    "\n",
    "$$ K=4 \\rightarrow P_{error} = P_{A} \\times  P_{B} =  \\frac{1}{2} \\times \\frac{1.5}{4} = 0.1875  $$\n",
    "\n",
    "**when K=4**, the test point A will be classified rightly or misclassified with 2 in out of 4 points is clearly misclassified. Totally, it would have P(A)= $\\frac{2}{4}$ =$\\frac{1}{2}$. In addition, the test point B will be classified rightly or misclassified with 1 in out of 4 points is clearly misclassified and the other 1 point could be classified as class 1 or class 2, hence 0.5/4 chance of misclassification. Totally, it would have P(B)= $\\frac{1+0.5}{4}$ =$\\frac{1.5}{4}$.  Thus, the joint probability of misclassification is P=$\\frac{1}{2}\\cdot\\frac{1.5}{4}=0.1875$, when K=4\n",
    "\n",
    "$$ K=5 \\rightarrow P_{error} = P_{A} \\times  P_{B} =  \\frac{3}{5} \\times \\frac{2}{5} = 0.24  $$\n",
    "\n",
    "**when K=5**, the test point A will be classified rightly or misclassified, and the P error (A)=3/5. The test point B has the same distance to Class 1 and Class 2, so it could either be classified rightly or misclassified, then P error (B)=2/5. Thus, the P=$\\frac{3}{5}\\cdot \\frac{2}{5}=0.24$, when K=5\n",
    "\n",
    "Hence, the optimal number of K is **K=3** due to its lowest misclassification probability. The fact that we have to find the product of two different probability in our calculation is due to the fact that we want to know the joint proability of misclassification for two different test point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8t9u_kDpgTD2"
   },
   "source": [
    "## 2. [Naive Bayes Classifier, 6 points]\n",
    "\n",
    "### Exercise 2.1 (3 pts)\n",
    "A psychologist does a small survey on ''happiness''. Each respondent provides a vector with entries 1 or 0 corresponding to if they answered “yes” or “no” to a question respectively. The question vector has attributes \n",
    "$$\n",
    "x = (\\mbox{rich, married, healthy}) \\tag{1}\n",
    "$$\n",
    "\n",
    "Thus a response $(1, 0, 1)$ would indicate that the respondent was\n",
    "''rich'', ''unmarried'' and ''healthy''. In addition, each respondent\n",
    "gives a value $c = 1$ if they are content wih their life and $c = 0$\n",
    "if they’re not. The following responses were obtained.\n",
    "\n",
    "$$\n",
    "c = 1: (1, 1, 1),(0, 0, 1),(1, 1, 0),(1, 0, 0) \\\\\n",
    "c = 0: (0, 0, 0),(1, 0, 1),(0, 0, 1),(0, 1, 0)\n",
    "$$\n",
    "\n",
    "1. Using naive Bayes, what is the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content''? **(1 pt)**\n",
    "\n",
    "2. What is the probability that a person who is ''not rich'' and ''married'' is content ? (i.e. we do not know if they are ''healthy'') **(2 pts)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aEEeDnaN1Ikp"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "\n",
    " **1.** Using Naive Bayes, the posterior probability formulation without normalizing constant is as shown below\n",
    "\n",
    "$$  P(x_{1},...,x_{n}|C) =  \\prod_{i=1}^{n} P(x_{i}|C) $$\n",
    "\n",
    "$$ P(C|x_{1},...,x_{n}) = P(x_{1},...,x_{n}|C) \\times P(C) = \\underbrace{\\prod_{i=1}^{n} P(x_{i}|C)}_\\text{Likelihood} \\times \\underbrace{P(C)}_\\text{Prior}  $$\n",
    "where\n",
    "\n",
    "$$ x_{1} \\rightarrow \\{rich\\}, x_{2} \\rightarrow \\{married\\}, x_{3} \\rightarrow \\{healthy\\}$$\n",
    "\n",
    "$$ P(C) \\rightarrow \\text{Prior Probability of being content} = 0.5 $$\n",
    "\n",
    "$$ P(x_{i}|C) \\rightarrow  \\text{Conditional Probability of being content if certain independent variable (e.g.'rich','married','healthy') is satisfied} $$\n",
    "\n",
    "$$ P(x_{1},...,x_{n}|C) \\rightarrow \\text{Likelihood/joint probability of being}\\;x_{i}\\;\\text{if a person is being content} $$\n",
    "\n",
    "$$P(C|x_{1},...,x_{n}) \\rightarrow \\text{Posterior/ probability that a person to be content if a person is}\\;x_{i}$$\n",
    "\n",
    "Considering the problem that we have above, we could create the table to illustrate the calculation process\n",
    "\n",
    "![task2 1](https://user-images.githubusercontent.com/58375350/80309629-86a60600-87d6-11ea-8d39-6244f8127d9b.png)\n",
    "\n",
    "\n",
    "By observing the table shown above and using the equation that is defined, the probability that a person is ''not rich'', ''married'' and ''healthy'' is ''content'' is\n",
    "\n",
    "$$ P(\\text{content}|\\text{not rich, married, healthy}) = \\prod_{i=1}^{n} P(x_{i}|C) \\times P(C) = P(\\text{not rich}| \\text{content}) \\cdot P(\\text{married}|\\text{content}) \\cdot P(\\text{healthy} |\\text{content}) \\cdot P(\\text{content}) $$\n",
    "\n",
    "$$= 0.25 \\cdot 0.5 \\cdot 0.5 \\cdot 0.5 =  0.03125   $$\n",
    "\n",
    "**2.** Moreover, the probability that a person is ''not rich'' and ''married'' is content  is\n",
    "\n",
    "$$ P(\\text{content}|\\text{not rich, married}) =  \\prod_{i=1}^{n} P(x_{i}|C) \\times P(C) = P(\\text{not rich}| \\text{content})  \\cdot P(\\text{married}|\\text{content}) \\cdot P(\\text{content}) =  0.25\\cdot 0.5 \\cdot 0.5 = 0.0625 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8FkqFQkN1LWx"
   },
   "source": [
    "### Exercise  2.2 (3 pts)\n",
    "Naive Bayes refers to the classifier which we now describe. We consider here **binary** classification problem with **real valued data** i.e. $x \\in \\mathbb{R}^2$.\n",
    "#### 1. (1 pt)\n",
    "Assume that the class conditional density is **spherical** Gaussian, that is, the likelihood of the training(and testing) data $X, y$ given class $i$ can be expressed as: \n",
    "\n",
    "$$ \n",
    "P( x_{new} | y_{new}, X, y) = P(x_{new} | \\tag{1}\n",
    "\\mu_{i}, \\Sigma_{i})\n",
    "$$\n",
    "\n",
    "Assume both classes have equal prior $p(y= \\pm 1) = 0.5$. Write the expression for the **naive Bayes** classifier, that is, derive\n",
    "$$\n",
    "P(y_{new} = -1 | x_{new} , X, y ) \\\\ \\tag{2}\n",
    "P(y_{new} = +1 | x_{new} , X, y ) ~.\n",
    "$$\n",
    "\n",
    "***Hint***: Derive the expressions of MLE for parameters in terms of training-data. Then express eq.2 in terms of those estimates using Bayes rule. \n",
    "\n",
    "#### 2. (2 pts)\n",
    "Derive the MLE expression for parameters when the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown scalars. This is done to alleviate \"naive\" assumption, since now feature components are no longer independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyg8jlaj1Nb8"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "$ 1.$\n",
    "Assume we have $\\textbf{n}$ random vectors, each of size p: $X^{(1)},X^{(2)},...X^{(n)}$ where each random vectors can be interpreted as data point across p variables. If each $X^{(i)}$ are independent and identically distributed random variables as multivariate Gaussian vectors : \n",
    "\n",
    " $$X^{(i)} \\sim \\mathcal{N}(\\mu_{kd},\\,\\sigma^{2}_{kd})$$\n",
    " \n",
    "In order to find the MLE of the parameters, we could try to find the log-likelihood of a multivariate gaussian first as below. \n",
    " $$l(\\mu,\\Sigma|X^{(i)}) = \\prod_{i = 1}^{n}f_{X^{(i)}}(X^{(i)}|\\mu,\\Sigma) = log\\prod_{i=1}^{n}\\frac{1}{(2\\pi)^{p/2}|\\Sigma|^{1/2}}e^{-\\frac{1}{2}{(X^{(i)}-\\mu_{i})}^{T}\\Sigma^{-1}(X^{(i)}-\\mu_{i})}$$ \n",
    " \n",
    " $$=\\Sigma_{i=1}^{n}(-\\frac{p}{2}log(2\\pi) - \\frac{1}{2}log|\\Sigma_{i}| - \\frac{1}{2}(X^{(i)} - \\mu_{i})^{T}\\Sigma_{i}^{-1}(X^{(i)} - \\mu_{i}))$$\n",
    " \n",
    " $$ l(\\mu, \\Sigma;) = - \\frac{2p}{2}log(2\\pi) - \\frac{n}{2}log|\\Sigma_{i}| - \\frac{1}{2}\\Sigma_{i=1}^{n}(X^{(i)} - \\mu_{i})^{T}\\Sigma_{i}^{-1}(X^{(i)} - \\mu_{i})$$\n",
    " \n",
    " Then, by defining $\\Sigma_{i} = \\sigma^2_{kd}$, the absolute value of the symmetric and diagonal covariance matrix, which is equal to the determinant of $\\Sigma$ \n",
    "\n",
    "$$|\\Sigma_{i}| = |\\sigma^2_{kd}| = det(\\sigma^2_{kd}) = (\\sigma^2_{kd})^{p}$$\n",
    "\n",
    "$$l(\\mu_{kd},\\sigma^2_{kd}) = - p\\cdot log(2\\pi) -  \\frac{np}{2}\\cdot log(\\sigma^2_{kd}) - \\frac{1}{2\\sigma^2_{kd}} \\Sigma_{i=1}^{D}(X^{(i)} - \\mu_{kd})^{T}(X^{(i)} - \\mu_{kd})$$\n",
    "\n",
    "Partial derivative of the log-likelihood function with respect to $\\sigma^{2}$ will give\n",
    "\n",
    "$$\\frac{\\partial l}{\\partial \\sigma_{kd}^2} = -\\frac{np}{2\\sigma^2_{kd}}+ \\frac{1}{2(\\sigma^{2}_{kd})^{2}}\\Sigma_{i=1}^{n}(X^{(i)} - \\mu_{kd})^{T}(X^{(i)} - \\mu_{kd})$$\n",
    "\n",
    "Setting the equation equal to zero and since we have 2 variables of $x \\in \\mathbb{R}^2 $, therefore it will give us $p=2$ and the equation becomes\n",
    "\n",
    "$$\\hat{\\sigma}_{kd}^{2} = \\frac{1}{2n}\\Sigma_{i=1}^{n}(X^{(i)} - \\hat{\\mu_{kd}})^{T}(X^{(i)} - \\hat{\\mu_{kd}})$$ \n",
    "\n",
    "Then, from inserting the mean estimation below, the estimation of the variance becomes\n",
    "\n",
    "$$\\hat{\\sigma}_{kd}^{2} = \\frac{1}{2n}\\Sigma_{i=1}^{n}(X^{(i)} - \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}(X^{(i)} - \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})$$ \n",
    "\n",
    "$$[\\textbf{Maximum Likelihood Estimator of}\\;\\sigma^{2}]$$\n",
    "\n",
    "For the mean $\\mu$, we could find it by derivation of the first equation above with respect to $\\mu$ as below\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\mu} l(\\mu,\\Sigma|X^{(i)}) = \\Sigma^{n}_{i=1}\\Sigma^{-1}(\\mu - X^{(i)}) = 0 $$\n",
    "\n",
    "Due to the fact that we assume that we are using non-degenerate multivariate normal distribution, hence $\\Sigma$ is $\\textbf{positive definite}$. Therefore, the equation above becomes\n",
    "\n",
    "$$0 = n\\mu - \\Sigma_{i=1}^{n} X^{(i)}$$\n",
    "\n",
    "$$\\hat{\\mu}_{kd} = \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)} = \\bar{X}$$\n",
    "\n",
    "$$[\\textbf{Maximum Likelihood Estimator of}\\;\\mu]$$\n",
    "\n",
    "Then, after getting MLE of both parameters, we express equation (2) as defined in the problem in terms of the MLE using Bayes rule. By assuming that the normalizing term is negligible, then mathematically, it could be expressed as\n",
    "\n",
    "$$ P(y_{new} = -1 | x_{new} , X, y ) = P(x_{new} | y_{new} = -1, \\mu_{i}, \\Sigma_{i})\\cdot p(y_{new}=-1)$$\n",
    "\n",
    "$$   =  P(x_{new} | \\mu_{i}, \\Sigma_{i}) \\cdot {0.5} $$\n",
    "\n",
    "$$ = \\left(\\prod_{i=1}^{D}\\frac{1}{(2\\pi)\\left(\\frac{1}{2n}\\Sigma_{i=1}^{n}(X^{(i)} - \\mu_{kd})^{T}(X^{(i)} - \\mu_{kd})\\right)}exp\\{-\\frac{1}{2}{(X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}}\\Sigma^{-1}(X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})\\}\\right) \\cdot 0.5 $$\n",
    "\n",
    "$$ = \\underbrace{\\left(\\frac{n}{\\pi}\\cdot\\prod_{i=1}^{D}\\frac{1}{\\left(\\Sigma_{i=1}^{n}(X^{(i)} -  \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}(X^{(i)} -  \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})\\right)}exp\\{-\\frac{1}{2}{(X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}}\\Sigma^{-1}(X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})\\}\\right)}_\\text{Likelihood} \\cdot \\underbrace{0.5}_\\text{prior} $$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\Sigma_{i}^{-1} = \\begin{bmatrix}\n",
    "\\frac{1}{\\sigma^{2}_{kd,1}} & & \\\\ \n",
    "& \\ddots & \\\\ \n",
    "& & \\frac{1}{\\sigma^{2}_{kd,p}}\\\\\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "$$ = \\left(\\frac{n}{2\\pi}\\cdot\\prod_{i=1}^{D}\\frac{1}{\\left(\\Sigma_{i=1}^{n}(X^{(i)} -  \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}(X^{(i)} -  \\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})\\right)}exp\\{-\\frac{1}{2}{(X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})^{T}}\\begin{bmatrix}\n",
    "\\frac{1}{\\sigma^{2}_{kd,1}} & & \\\\ \n",
    "& \\ddots & \\\\ \n",
    "& & \\frac{1}{\\sigma^{2}_{kd,p}}\\\\\n",
    "\\end{bmatrix} (X^{(i)}-\\frac{1}{n}\\Sigma_{i=1}^{n} X^{(i)})\\}\\right)  $$ \n",
    "\n",
    "Since $X\\in \\mathbb{R}^{2}$, the equation above becomes\n",
    "\n",
    "$$ = \\frac{n}{2\\pi}  \\frac{1}{\\left(\n",
    " {\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\n",
    "}^{T}\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\\right)}exp\\left\\{-\\frac{1}{2}{\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\n",
    "}^{T}\\begin{bmatrix} \\frac{1}{\\sigma_{kd,1}^{2}} & \\\\\n",
    " & \\frac{1}{\\sigma_{kd,2}^{2}}\\\\\n",
    "\\end{bmatrix}{\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\n",
    "}\\right\\} $$ \n",
    "\n",
    "$$ = \\frac{n}{2\\pi}   \\frac{1}{\\left(\n",
    " {\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\n",
    "}^{T}\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\\right)}exp\\left\\{-\\frac{1}{2}{\\begin{bmatrix} X_{1}-\\mu_{1}\\\\X_{2}-\\mu_{2}\\\\\\end{bmatrix}\n",
    "}^{T} {\\begin{bmatrix} \\frac{1}{\\sigma_{kd,1}^{2}}(X_{1}-\\mu_{1})\\\\\\frac{1}{\\sigma_{kd,2}^{2}}(X_{2}-\\mu_{2})\\\\\\end{bmatrix}\n",
    "}\\right\\} $$ \n",
    "\n",
    "$$ =\\frac{n}{2\\pi}   \\frac{1}{(X_{1}-\\mu_{1})^{2}+(X_{2}-\\mu_{2})^{2}}exp\\left\\{-\\frac{1}{2\\sigma_{kd,1}^{2}}(X_{1}-\\mu_{1})^{2}-\\frac{1}{2\\sigma_{kd,2}^{2}}(X_{2}-\\mu_{2})^{2}\\right\\} $$ \n",
    "\n",
    "$$ =    \\frac{n}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}\\left(\\frac{1}{\\sqrt{2\\pi}}exp\\left\\{-\\frac{1}{2\\sigma_{kd,1}^{2}}(X_{1}-\\mu_{1})^{2}\\right\\}\\cdot \\frac{1}{\\sqrt{2\\pi}}  exp\\left\\{-\\frac{1}{2\\sigma_{kd,2}^{2}}(X_{2}-\\mu_{2})^{2}\\right\\}\\right) $$ \n",
    "\n",
    "Since n denotes number of random vectors, and we have it as 2, then the equation becomes\n",
    "\n",
    "$$=  \\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}\\left(\\frac{1}{\\sqrt{2\\pi}} exp\\left\\{-\\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}(X_{1}-\\bar{X_{1}})^{2}\\right\\}\\cdot \\frac{1}{\\sqrt{2\\pi}}  exp\\left\\{-\\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}(X_{2}-\\bar{X_{2}})^{2}\\right\\}\\right) $$ \n",
    "\n",
    "The fact that we have the same prior probability will finally give us similar naive Bayes Classifier\n",
    "\n",
    "$$ P(y_{new} = +1 | x_{new} , X, y ) = P(x_{new} | y_{new} = +1, \\mu_{i}, \\Sigma_{i})\\cdot p(y_{new}=+1)$$\n",
    "\n",
    "$$   = P(x_{new} | y_{new} = +1, \\mu_{i}, \\Sigma_{i})\\cdot 0.5 =  P(x_{new} | \\mu_{i}, \\Sigma_{i})\\cdot 0.5$$\n",
    "\n",
    "$$=  \\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}\\left(\\frac{1}{\\sqrt{2\\pi}} exp\\left\\{-\\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}(X_{1}-\\bar{X_{1}})^{2}\\right\\}\\cdot \\frac{1}{\\sqrt{2\\pi}}  exp\\left\\{-\\frac{2}{(X_{1}-\\bar{X_{1}})^{2}+(X_{2}-\\bar{X_{2}})^{2}}(X_{2}-\\bar{X_{2}})^{2}\\right\\}\\right) $$ \n",
    "\n",
    "Note that the Naive-Bayes assumption stated that the components of $x_{new}$ is independent for a particular class. Using Naive-Bayes, the number of parameters needed to be estimated is significantly less than without Naive Bayes as will be shown in the next question.\n",
    "\n",
    "**(2)** Assume that the covariance matrix is not diagonal, i.e, Covariance matrix has 4 unknown non-zero scalars as shown below\n",
    "\n",
    "$$ \\Sigma =  \\begin{bmatrix} \\sigma_{x}^{2} & \\rho\\sigma_{x}\\sigma_{y}\\\\\n",
    "\\rho\\sigma_{y}\\sigma_{x} & \\sigma_{y}^{2}\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The off-diagonal covariance values includes correlation coefficient $\\rho$ which tells us that some of the components are non-independent, e.g. they vary with respect to one another. \n",
    "Suppose that we have\n",
    "\n",
    "$$ (\\text{det}\\;\\Sigma)^{1/2} = (\\sigma_{x}^{2}\\sigma_{y}^{2}-\\rho^{2}\\sigma_{x}^{2}\\sigma_{y}^{2})^{1/2}=\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}$$\n",
    "\n",
    "$$\\Sigma^{-1} = \\frac{1}{ (\\sigma_{x}^{2}\\sigma_{y}^{2}-\\rho^{2}\\sigma_{x}^{2}\\sigma_{y}^{2})}\\begin{bmatrix} \\sigma_{y}^{2} & -\\rho\\sigma_{x}\\sigma_{y}\\\\\n",
    "-\\rho\\sigma_{y}\\sigma_{x} & \\sigma_{x}^{2}\\\\ \n",
    "\\end{bmatrix}$$\n",
    "\n",
    "The joint density function of a bivariate normal with correlation coefficient ρ is\n",
    "\n",
    "$$P(x_{new} | \\mu_{i}, \\Sigma_{i}) = \\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}exp\\left(-\\frac{1}{2}(x-\\mu)^{T}\\Sigma^{-1}(x-\\mu)\\right) =  \\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}exp\\left(-\\frac{1}{2}\\frac{1}{\\sigma_{x}^{2}\\sigma_{y}^{2}(1-\\rho^{2})}\n",
    "\\begin{bmatrix}\n",
    "\\sigma_{y}^{2}(x-\\mu_{x})-\\rho\\sigma_{x}\\sigma_{y}(y-\\mu_{y})\\\\\n",
    "-\\rho\\sigma_{x}\\sigma_{y}(x-\\mu_{x})+\\sigma_{x}^{2}(y-\\mu_{y})\\\\\n",
    "\\end{bmatrix}^{T}\n",
    "\\begin{bmatrix}\n",
    "x-\\mu_{x}\\\\\n",
    "y-\\mu_{y}\\\\\n",
    "\\end{bmatrix}\n",
    "\\right) = \\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}exp\\left(-\\frac{1}{2\\sigma_{x}^{2}\\sigma_{y}^{2}(1-\\rho^{2})}\n",
    "\\sigma_{y}^{2}(x-\\mu_{x})^{2}-2\\rho\\sigma_{x}\\sigma_{y}(x-\\mu_{x})(y-\\mu_{y})+\\sigma^{2}_{x}(y-\\mu_{y})^{2}\n",
    "\\right) =  \\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}exp\\left(-\\frac{1}{2(1-\\rho^{2})}\\left(\\frac{(x-\\mu_{x})^{2}}{\\sigma_{x}^{2}}-2\\rho\\frac{(x-\\mu_{x})(y-\\mu_{y})}{\\sigma_{x}\\sigma_{y}}+\\frac{(y-\\mu_{y})^{2}}{\\sigma_{y}^{2}}\\right)\\right)\n",
    "$$ \n",
    "\n",
    "Simplifying the equation above by exchanging the term inside the exponential with the coefficient r gives\n",
    "\n",
    "$$ P(x_{new} | \\mu_{i}, \\Sigma_{i}) =  \\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}exp\\left(-\\frac{r}{2}\\right)    $$\n",
    "\n",
    "The likelihood of the equation above will look like below\n",
    "\n",
    "$$ L(\\theta|x,y) =  \\left(\\frac{1}{(2\\pi)\\left(\\sigma_{x}\\sigma_{y}\\sqrt{1-\\rho^{2}}\\right)}\\right)^{n}exp\\left(-\\frac{1}{2}\\Sigma_{i=1}^{n}r_{i} \\right)$$\n",
    "\n",
    "where $\\theta = (\\mu_{1},\\mu_{2},\\sigma_{1}^{2}.\\sigma_{2}^{2},\\rho),\\;\\; x = (x_{1},....,x_{n}),\\;\\; y=(y_{1},....,y_{n})$.\n",
    "\n",
    "In this case, the log-likelihood could be written as,\n",
    "\n",
    "$$ \\lambda(\\theta) = \\lambda(\\theta|x,y) = log L(\\theta|x,y) = -n\\cdot \\text{log}(2\\pi)-\\frac{n}{2}\\cdot\\text{log}(\\sigma_{x}^{2})-\\frac{n}{2}\\cdot\\text{log}(\\sigma_{y}^{2})-\\frac{n}{2}\\text{log}(1-\\rho^{2})-\\frac{1}{2}\\Sigma_{i=1}^{n}(r_{i})  $$\n",
    "\n",
    "Taking the derivative with respect to the parameters $\\theta$ will give us\n",
    "\n",
    "$$ \\frac{\\partial\\lambda(\\theta)}{\\partial\\mu_{x}} = \\frac{n}{\\sigma_{x}^{2}(1-\\rho^{2})}(\\bar{x}-\\mu_{x})-\\frac{n\\rho}{\\sigma_{x}\\sigma_{y}(1-\\rho^{2})}(\\bar{y}-\\mu_{y})$$\n",
    "\n",
    "$$ \\frac{\\partial\\lambda(\\theta)}{\\partial\\mu_{y}} = \\frac{n}{\\sigma_{y}^{2}(1-\\rho^{2})}(\\bar{y}-\\mu_{y})-\\frac{n\\rho}{\\sigma_{x}\\sigma_{y}(1-\\rho^{2})}(\\bar{x}-\\mu_{x})$$\n",
    "\n",
    "$$\\frac{\\partial\\lambda(\\theta)}{\\partial\\sigma_{x}^{2}} = -\\frac{n}{2\\sigma_{x}^{2}} + \\frac{\\Sigma_{i=1}^{n}(x_{i}-\\mu_{x})^{2}}{2\\sigma_{x}^{4}(1-\\rho^{2})}-\\frac{\\rho\\Sigma_{i=1}^{n}(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{2\\sigma_{x}^{3}\\sigma_{y}(1-\\rho^{2})}$$\n",
    "\n",
    "$$\\frac{\\partial\\lambda(\\theta)}{\\partial\\sigma_{x}^{2}} = -\\frac{n}{2\\sigma_{y}^{2}} + \\frac{\\Sigma_{i=1}^{n}(y_{i}-\\mu_{y})^{2}}{2\\sigma_{y}^{4}(1-\\rho^{2})}-\\frac{\\rho\\Sigma_{i=1}^{n}(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{2\\sigma_{x}\\sigma_{y}^{3}(1-\\rho^{2})}$$\n",
    "\n",
    "$$\\frac{\\partial\\lambda(\\theta)}{\\partial\\rho} = \\frac{n\\rho}{1-\\rho^{2}} - \\frac{\\rho\\Sigma_{i=1}^{n}(x_{i}-\\mu_{x})^{2}}{\\sigma_{x}^{2}(1-\\rho^{2})}-\\frac{\\rho\\Sigma_{i=1}^{n}(y_{i}-\\mu_{y})^{2}}{\\sigma_{y}^{2}(1-\\rho^{2})}+ \\frac{(1+\\rho^{2})\\Sigma_{i=1}^{n}(x_{i}-\\mu_{x})(y_{i}-\\mu_{y})}{\\sigma_{x}\\sigma_{y}(1-\\rho^{2})^{2}}$$\n",
    "\n",
    "Setting $\\frac{\\partial\\lambda(\\theta)}{\\partial\\mu_{x}}=\\frac{\\partial\\lambda(\\theta)}{\\partial\\mu_{y}} = 0$ and solving for $\\mu_{x}$ and $\\mu_{y}$, we could get the following as the estimate\n",
    "\n",
    "$$ \\hat{\\mu}_{x} = \\bar{x},\\;\\;\\; \\hat{\\mu}_{y} = \\bar{y}$$\n",
    "\n",
    "Moreover, by setting $\\frac{\\partial\\lambda(\\theta)}{\\partial\\sigma_{x}^{2}}=\\frac{\\partial\\lambda(\\theta)}{\\partial\\sigma_{y}^{2}} = \\frac{\\partial\\lambda(\\theta)}{\\partial\\rho} = 0$ and replacing $\\mu_{x}$ and $\\mu_{y}$ by the respective expressions $\\bar{x}$ and $\\bar{y}$, we would get the following for the estimates of covariance and correlation\n",
    "\n",
    "$$ \\hat{\\sigma}_{x}^{2} = \\frac{1}{n}\\Sigma_{i=1}^{n}(x_{i}-\\bar{x})^{2}$$\n",
    "\n",
    "$$ \\hat{\\sigma}_{y}^{2} = \\frac{1}{n}\\Sigma_{i=1}^{n}(y_{i}-\\bar{y})^{2}$$\n",
    "\n",
    "$$ \\hat{\\rho} = \\frac{\\hat{\\sigma}_{x}\\hat{\\sigma}_{y}}{\\left(\\hat{\\sigma}_{x}^{2}\\hat{\\sigma}_{y}^{2}\\right)^{\\frac{1}{2}}} = \\frac{\\frac{1}{n}\\Sigma_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\left(\\frac{1}{n}\\Sigma_{i=1}^{n}(y_{i}-\\bar{y})^{2}\\right)^{\\frac{1}{2}}\\left(\\frac{1}{n}\\Sigma_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\right)^{\\frac{1}{2}}} $$\n",
    "\n",
    "From the process that we have done above, it is shown that without using the Naive-Bayes assumption as well assumption that the covariance matrix is diagonal will in fact makes it hard for us to find the estimation of the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "## 3. [SVM, 5 points]\n",
    "\n",
    "### Excercise 3.1 (2 pts)\n",
    "\n",
    "Consider a (hard margin) SVM with the following training points from\n",
    "two classes:\n",
    "\\begin{eqnarray}\n",
    "+1: &(2,2), (4,4), (4,0) \\nonumber \\\\\n",
    "-1: &(0,0), (2,0), (0,2) \\nonumber\n",
    "\\end{eqnarray}\n",
    "\n",
    "1. Plot these six training points, and construct, by inspection, the\n",
    "weight vector for the optimal hyperplane. **(1 pt)**\n",
    "\n",
    "2. In your solution, specify the hyperplane in terms of w and b such that $w_1 x_1 + w_2 x_2 + b =0$. Calculate the margin, i.e. $2\\gamma$, where $\\gamma$ is the\n",
    "distance from the hyperplane to its closest data point. (Hint: It may be useful to recall that the distance of a point $(a_1,a_2)$ from the line $w_1x_1 + w_2x_2 + b = 0$ is $|w_1a_1 + w_2a_2 + b|/\\sqrt{w_1^2 + w_2^2}$.) **(1 pt)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZWt7B2y1Qqx"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "**1.** The plot for the six training points could be illustrated as below\n",
    "\n",
    "![task3 1 1](https://user-images.githubusercontent.com/58375350/80309718-0c29b600-87d7-11ea-852a-9b6b48b08615.png)\n",
    "\n",
    "By inspection, the weight vector for the optimal hyperplane could be constructed as following\n",
    "\n",
    "![task3 1 2](https://user-images.githubusercontent.com/58375350/80309826-98d47400-87d7-11ea-8731-736ea3dab497.png)\n",
    "\n",
    "From the illustration above, we could obtain the **decision boundary** depicted as the **green line**. Moroever, by visual inspection, we could obtain the following conclusion :\n",
    "\n",
    "(a).The decision boundary could be expressed mathematically by linear equation as $ x_{2} = x_{1} + 3 $\n",
    "\n",
    "(b). Support vector for class = +1, could be expressed mathematically by linear equation as $ x_{2} = -x_{1} +4 $\n",
    "\n",
    "(c). Support vector for class = -1, could be expressed mathematically by linear equation as $ x_{2} = -x_{1} +2 $\n",
    "\n",
    "**2.** \n",
    "\n",
    "Suppose that We know that the general equation  is defined as  \n",
    "\n",
    "$$w_1 x_1 + w_2 x_2 + b =0$$\n",
    "\n",
    "We know from **part 1** that the  decision boundary could be expressed mathematically by linear equation as \n",
    "\n",
    "$$ x_{2} = -x_{1} + 3 $$\n",
    "\n",
    "which could also be expressed as \n",
    "\n",
    "$$x_{2} + x_{1} - 3 = 0 $$\n",
    "\n",
    "Thus, we would get the following coefficients by comparing the general equation and the decision boundary.\n",
    "\n",
    "$$ w_1 = 1,\\;\\; w_2 = 1,\\;\\; b = -3 $$\n",
    "\n",
    "Suppose that we take one of the vector point from positive class (+),\n",
    "\n",
    "$$ a_{1} = 2,\\;\\; a_{2} = 2 $$\n",
    "\n",
    "We could calculate the distance of the point **($a_1,a_2$)** from the line $x_{2} + x_{1} - 3 = 0 $ denoted as $\\gamma$ as\n",
    "\n",
    "$$  \\gamma = \\frac{|w_{1}a_{1}+w_{2}a_{2}+b|}{\\sqrt{w_{1}^{2}+w_{2}^{2}}}  $$\n",
    "\n",
    "$$  \\gamma = \\frac{|(1)(2)+(1)(2)-3|}{\\sqrt{(1)^{2}+(1)^{2}}}  $$\n",
    "\n",
    "$$  \\gamma = \\frac{|2+2-3|}{\\sqrt{2}}  $$\n",
    "\n",
    "$$ \\gamma = \\frac{|1|}{\\sqrt{2}}$$\n",
    "\n",
    "$$ \\gamma = \\frac{1}{2}\\sqrt{2} $$\n",
    "\n",
    "Hence the margin could be calculated as \n",
    "\n",
    "$$ \\text{Margin} = 2\\gamma = 2\\cdot\\frac{1}{2}\\sqrt{2} = \\sqrt{2}  $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_3iVQypENwJQ"
   },
   "source": [
    "\n",
    "\n",
    "### Excercise 3.2 (3 pts)\n",
    "\n",
    "Consider the same problem from above.\n",
    "\n",
    "1. Write the primal formulation of the SVM **for this specific example** i.e. you have to specialise the general formulation for the set of inputs given. **(1 pt)**\n",
    "\n",
    "2. Write the dual formulation **for this specific**. Give the optimal dual solution, comment on support vectors. **(2 pts)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DZWt7B2y1Qqx"
   },
   "source": [
    "### Your answer here:\n",
    "\n",
    "\n",
    "\n",
    "**1.** From above, the decision boundary could be expressed as \n",
    "\n",
    "$$ x_{1}+x_{2}-3 = 0 $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ \\mathbf{w} = \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} \\rightarrow \\mathbf{w}^{T}\\mathbf{w} =\\begin{bmatrix}\n",
    "1 & 1\\\\\n",
    "\\end{bmatrix} \\cdot \\begin{bmatrix}\n",
    "1\\\\\n",
    "1\\\\\n",
    "\\end{bmatrix} = 2 $$\n",
    "\n",
    "Then, by definition, The SVM with the soft margin $\\xi_{i}$ can be generally formulated as an optimization problem as shown below\n",
    "\n",
    "$$\n",
    " \\underset{\\mathbf{w} \\in \\mathbb{R}^d,\\;\\xi_{i}\\in \\mathbb{R}}{\\text{min}} \\frac{1}{2}\\mathbf{w}^{T}\\mathbf{w} + C\\cdot\\Sigma_{i=1}^{N}\\xi_{i}\\;\\;\\;\\;\\;(1)\n",
    "  \\\\\n",
    " \\text{s.t}\n",
    "    \\;\\;y_{i}(\\mathbf{w}^{T}x_{i} + b) \\geq{1-\\xi_{i}}\\;\\text{where}\\;\\xi_{i}\\geq{0},\\; \\text{for}\\;i=1,...,N\\\\\n",
    "$$\n",
    "\n",
    "where C is the **regularization parameter**. Moreover, using the set of inputs from the **Exercise 3.1** as well as ($\\xi_{i} \\rightarrow \\text{soft margin}$), we could express the equation as \n",
    "\n",
    "$$  \\underset{\\xi_{i}\\in \\mathbb{R}}{\\text{min}}\\;\\; 1 + C\\cdot\\Sigma_{i=1}^{N}\\xi_{i}\\;\\;\\;\\;(2)\n",
    "  \\\\ \\text{s.t}\n",
    "    \\;\\;(x_{1}+x_{2}) \\leq{4-\\xi_{i}}\\\\\n",
    "   \\;\\;\\; \\;\\;(x_{1}+x_{2}) \\geq{2-\\xi_{i}}\\;  $$\n",
    "  \n",
    "where\n",
    "\n",
    "   $$ \\;\\xi_{i}\\;\\geq{0},\\; \\text{for}\\;i=1,...,N $$\n",
    "    \n",
    "    \n",
    " $$[\\textbf{Constrained Primal Form}]$$\n",
    "\n",
    "The constraint $y_{i}(\\mathbf{w}^{T}x_{i} + b)\\geq{1-\\xi_{i}}$ can be written more concisely as\n",
    "\n",
    "$$ y_{i}f(x_{i})\\geq{1-\\xi_{i}}$$\n",
    "\n",
    "which, together with $\\xi_{i}\\geq{0}$ is equivalent to\n",
    "\n",
    "$$ \\xi_{i} = \\text{max}(0,1-y_{i}f(x_{i})) = \\text{max}(0,1-y_{i}(x_{1}+x_{2}-3))  $$\n",
    "\n",
    "Hence the general formulation above is equivalent to the unconstrained optimization problem over\n",
    "\n",
    "$$  \\underset{\\xi_{i}\\in \\mathbb{R}}{\\text{min}}\\; 1 + C\\cdot\\Sigma_{i=1}^{N}\\text{max}(0,1-y_{i}(x_{1}+x_{2}-3)) \\;\\;\\;\\;(3)$$\n",
    "\n",
    "$$[\\textbf{Unconstrained Primal Form}]$$\n",
    "    \n",
    "where **d** is the dimension of feature vector x and **N** is number of training points.\n",
    "\n",
    "**2.** Following the constrained optimization problem with equation (2) above in the primal form, we could expand it by using the KKT condition in order to find the optimal dual solution.\n",
    "\n",
    "Finding for the lagrangian saddle point $\\underset{\\alpha}{\\text{max}}\\;\\underset{\\xi_{i}\\in \\mathbb{R}}{\\text{min}}\\;\\mathcal{L}(\\xi,\\alpha)$ with a lagrangian multiplier $\\alpha_{i}\\geq0$\n",
    "\n",
    "$$ \\mathcal{L}(\\xi,\\alpha)= 1 + C\\cdot\\Sigma_{i=1}^{N}\\xi_{i}-\\Sigma_{i=1}^{N}\\alpha_{i}(y_{i}(x_{1}+x_{2}-3)-1+\\xi_{i})$$\n",
    "\n",
    "Computing the gradient\n",
    "\n",
    "$$  \\frac{\\partial \\mathcal{L}(\\xi_{i},\\alpha)}{\\xi_{i}} = C + \\Sigma_{i=1}^{N} \\alpha_{i} $$\n",
    "\n",
    "\n",
    "Using the optimality condition\n",
    "\n",
    "$$  \\frac{\\partial \\mathcal{L}(\\xi_{i},\\alpha)}{\\xi_{i}} = 0 \\Longrightarrow  C+ \\Sigma_{i=1}^{N} \\alpha_{i} = 0 $$\n",
    "\n",
    "$$ C =  -\\Sigma_{i=1}^{N} \\alpha_{i} $$\n",
    "\n",
    "For the problems of the form :\n",
    "\n",
    "$$\n",
    "\\underset{\\mathbf{w}}{\\text{min}}\n",
    "f(\\mathbf{w})\\\\\n",
    "\\text{s.t}\n",
    "\\;\\;\\;g(\\mathbf{w})\\geq{0}\\\\\n",
    "\\;\\;\\;h(\\mathbf{w})=0\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "The Karoush Kuhn Tucker (KKT) conditions are :\n",
    "$$\n",
    "    \\nabla{f}(\\mathbf{w}^{*}) + \\Sigma^{n}_{i=1}\\nabla{g_{i}}(\\mathbf{w}^{*}){\\alpha^{*}_{i}}  = C+ \\Sigma_{i=1}^{N} \\alpha_{i} =  0 \\rightarrow \\text{Stationarity} \\;\\;\\;\\;(1) \n",
    "$$\n",
    "$$\n",
    "    \\alpha_{i}^{*}\\geq{0} \\rightarrow \\text{dual admissibility}\\;\\;\\;i = 1,...,n\\;\\;(2)\n",
    "$$\n",
    "$$ \n",
    "(x_{1}+x_{2}) \\leq{4-\\xi_{i}}\\;\\;\\text{and}\\;\\;(x_{1}+x_{2}) \\geq{2-\\xi_{i}}\\;  \\rightarrow \\text{primal admissibility}\\;\\;\\;i = 1,...,n\\;\\;(3)\n",
    "$$\n",
    "$$\n",
    "   \\alpha_{i}(y_{i}(x_{1}+x_{2}-3)-1+\\xi_{i}) = 0 \\rightarrow \\text{complementarity}\\;\\;\\; i = 1,...,n\\;\\;(4)\n",
    "$$\n",
    "\n",
    "Hence, the dual form could be represented as below after inputting the C value\n",
    "\n",
    "$$ \\underset{\\alpha}{\\text{max}}\\; 1 - \\Sigma_{i=1}^{N}\\alpha_{i}\\xi_{i}-\\Sigma_{i=1}^{N}\\alpha_{i}(y_{i}(x_{1}+x_{2}-3)-1+\\xi_{i})$$\n",
    "\n",
    "$$\\text{such that}\\;\\; \\alpha_{i}\\geq{0}$$\n",
    "\n",
    " $$[\\textbf{Constrained Dual Form}]$$\n",
    " \n",
    "The complementary condition will as a result split the data into two sets\n",
    "\n",
    "**(1)** $\\mathcal{A}$ which is the active constraint\n",
    "\n",
    "$$\\mathcal{A} = \\{i\\in [1,n]\\;|\\;\\begin{bmatrix}\n",
    "+1\\\\\n",
    "-1\\\\\n",
    "\\end{bmatrix}(w^{*^{T}}x_{i}+b^{*}) = \\begin{bmatrix}\n",
    "1-\\xi_{i}\\\\\n",
    "1-\\xi_{i}\\\\\n",
    "\\end{bmatrix}\\}\n",
    "$$\n",
    "\n",
    "**(2)**$\\mathcal{\\bar{A}}$ which is inactive constraint\n",
    "\n",
    "$$ \\text{if}\\; i\\neq\\mathcal{A},\\;\\alpha_{i}=0$$\n",
    "\n",
    "The Support vector machines (SVM) could be formulated as below to learn a linear classifier by solving an optimization problem over $\\alpha_{i}$ \n",
    "\n",
    "$$ f(x) = \\Sigma_{j=1}^{d}\\;w_{j}x_{j} = \\Sigma_{i=1}^{N}\\;\\alpha_{i}y_{i}(x_{i}^{T}x)+b $$\n",
    "\n",
    "Since the decision border **only depends on 4 points**, the equation above could be expressed as\n",
    "\n",
    "$$ f(x) = \\Sigma_{i=1}^{4}\\;\\alpha_{i}y_{i}(x_{i}^{T}x)+b $$\n",
    "\n",
    "Moreover, we can say the following regarding the value of $\\alpha_{i}$\n",
    "\n",
    " $$\\alpha_{i}\\neq 0\\;\\rightarrow \\textbf{Support Vector points}$$\n",
    " \n",
    " $$\\alpha_{i} = 0\\;\\rightarrow \\textbf{Not useful points}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zhHF2_-R00si"
   },
   "source": [
    "# Practical Question\n",
    "## 4. Logistic Regression (5 pts)\n",
    "### Customer churn with Logistic Regression\n",
    "A telecommunications company is concerned about the number of customers leaving their land-line business for cable competitors. They need to understand who is leaving. Imagine that you are an analyst at this company and you have to find out who is leaving and why.\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "We will use a telecommunications dataset for predicting customer churn. This is a historical customer dataset where each row represents one customer. The data is relatively easy to understand, and you may uncover insights you can use immediately. Typically it is less expensive to keep customers than acquire new ones, so the focus of this analysis is to predict the customers who will stay with the company.\n",
    "This data set provides information to help you predict what behavior will help you to retain customers. You can analyse all relevant customer data and develop focused customer retention programs.\n",
    "The dataset includes information about:\n",
    "*   Customers who left within the last month – the column is called Churn.\n",
    "*   Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies.\n",
    "*   Customer account information – how long they had been a customer, contract, payment method, paperless billing, monthly charges, and total charges.\n",
    "*   Demographic info about customers – gender, age range, and if they have partners and dependents.\n",
    "We will help you load and visualise the dataset as well as the preprocessing, you need to build up your logistic regression model step by step and do the prediction.\n",
    "*   **Remember, you are not allowed to use sklearn in modelling and predicting, you have to fill your code in the skeleton.** \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UMzOgQsCoJSN"
   },
   "source": [
    "**Hints**:\n",
    "- You compute $sigmoid( w^T x + b) = \\frac{1}{1 + e^{-(w^T x + b)}}$ .\n",
    "- You compute activation $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$.\n",
    "- You calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$.\n",
    "\n",
    "Here are the two formulas you will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$\n",
    "\n",
    "- You write down the optimization function. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.\n",
    "- In prediction, you calculate $\\hat{Y} = A = \\sigma(w^T X + b)$.\n",
    "- You may use np.exp, np.log(), np.dot(), etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QR5cEzzzOVba",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set: (7, 160) (1, 160)\n",
      "Test set: (7, 40) (1, 40)\n",
      "train accuracy: 77.5 %\n",
      "test accuracy: 72.5 %\n"
     ]
    }
   ],
   "source": [
    "## Load the dataset and read it\n",
    "import pandas as pd\n",
    "import pylab as pl\n",
    "import numpy as np\n",
    "import scipy.optimize as opt\n",
    "from sklearn import preprocessing\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve('https://raw.githubusercontent.com/BruceZHANG6/Imagesforuse/master/ChurnData.csv', 'ChurnData.csv')\n",
    "except urllib.error.HTTPError as ex:\n",
    "    print('Problem:', ex)\n",
    "    \n",
    "churn_df = pd.read_csv(\"ChurnData.csv\")\n",
    "churn_df.head()\n",
    "\n",
    "## Data pre-processing and selection\n",
    "## Train/Test dataset split\n",
    "churn_df = churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip',   'callcard', 'wireless','churn']]\n",
    "churn_df['churn'] = churn_df['churn'].astype('int')\n",
    "churn_df.head()\n",
    "\n",
    "X = np.asarray(churn_df[['tenure', 'age', 'address', 'income', 'ed', 'employ', 'equip']])\n",
    "y = np.asarray(churn_df['churn'])\n",
    "y = np.reshape(y, (np.asarray(churn_df['churn']).shape[0], 1))\n",
    "X = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=4)\n",
    "X_train=X_train.T\n",
    "X_test=X_test.T\n",
    "y_train=y_train.T\n",
    "y_test=y_test.T\n",
    "print ('Train set:', X_train.shape,  y_train.shape)\n",
    "print ('Test set:', X_test.shape,  y_test.shape)\n",
    "\n",
    "## Modeling and predicting\n",
    "\n",
    "# GRADED FUNCTION: sigmoid\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Return: s -- sigmoid(z)\n",
    "    \"\"\"\n",
    "    s = 1/(1+(np.exp(-z)))\n",
    "    \n",
    "    return s\n",
    "\n",
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized to 0\n",
    "    \"\"\"    \n",
    "    l = (dim,1)\n",
    "    w = np.zeros(l)\n",
    "    b = 0\n",
    "   \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "    \n",
    "    return w, b\n",
    "  \n",
    "# GRADED FUNCTION: grad_cost\n",
    "def grad_cost(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- data of size (number of features, number of examples)\n",
    "    Y -- true \"label\" vector\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \"\"\" \n",
    "    m = X.shape[1]\n",
    "    z = np.dot(np.transpose(w),X)+b\n",
    "    \n",
    "    A = sigmoid(z)\n",
    "    error = np.sum(Y*np.log(A) + (1-Y)*np.log(1-A)) \n",
    "    \n",
    "    cost = -1/m*error               \n",
    "    db = np.dot(1/m,np.sum(A-Y))\n",
    "    dw  = (np.dot(X,np.transpose(A-Y)))/m\n",
    "\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    " # GRADED FUNCTION: optimize\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \"\"\"\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = grad_cost(w,b,X,Y)\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w-dw*learning_rate\n",
    "        b = b-db*learning_rate\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)     \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}   \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "  \n",
    "# GRADED FUNCTION: predict\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
    "    z = np.dot(np.transpose(w),X)+b\n",
    "    A = sigmoid(z)\n",
    "    \n",
    "    for i in range(A.shape[1]):\n",
    "        \n",
    "        if A[0,i]<=0.5:\n",
    "            Y_prediction[0,i]=0;\n",
    "        if A[0,i]>0.5:\n",
    "            Y_prediction[0,i]=1\n",
    "    \n",
    "    assert(Y_prediction.shape == (1, m))\n",
    "    \n",
    "    return Y_prediction\n",
    "  \n",
    "# GRADED FUNCTION: model\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations = 2000, learning_rate = 0.5, print_cost = False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    Returns: d -- dictionary containing information about the model.\n",
    "    \"\"\"    \n",
    "    # initialize parameters with zeros\n",
    "    dim = 7\n",
    "    w, b = initialize_with_zeros(dim)\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost = False)\n",
    "    \n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    Y_prediction_test = predict(w,b,X_test)\n",
    "    Y_prediction_train = predict(w,b,X_train)\n",
    "\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "   \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d\n",
    "\n",
    "## The train accuracy and test accuracy\n",
    "## Feel free to change the hyperparameters\n",
    "d = model(X_train, y_train, X_test, y_test, num_iterations = 20000, learning_rate = 0.005, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW3_2020.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
